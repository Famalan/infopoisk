\documentclass[a4paper,12pt]{article}

% Настройки языка и шрифтов
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{indentfirst}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{float}

\geometry{left=3cm,right=1.5cm,top=2cm,bottom=2cm}

\begin{document}

% Титульный лист
\begin{titlepage}
    \centering
    {\fontsize{14pt}{16pt}\selectfont МОСКОВСКИЙ АВИАЦИОННЫЙ ИНСТИТУТ\\(НАЦИОНАЛЬНЫЙ ИССЛЕДОВАТЕЛЬСКИЙ УНИВЕРСИТЕТ)}\\[0.5cm]
    {\fontsize{12pt}{14pt}\selectfont Институт №8 «Компьютерные науки и прикладная математика»}\\[4cm]
    
    {\fontsize{16pt}{18pt}\selectfont \textbf{Лабораторные работы}}\\
    {\fontsize{14pt}{16pt}\selectfont \textbf{по курсу «Информационный поиск»}}\\[5cm]
    
    \vfill
    \begin{flushright}
        \begin{minipage}{0.51\textwidth}
            Выполнил: Марков Владимир Игоревич\\
            Группа: М8О-408Б-22\\
            Преподаватель: Кухтичев Антон Алексеевич
        \end{minipage}
    \end{flushright}
    
    \vfill
    {\large Москва, 2025}
\end{titlepage}

\tableofcontents
\newpage

\section{Введение}
В рамках курса «Информационный поиск» была разработана полнофункциональная поисковая система, включающая подсистемы сбора данных (краулер), обработки текста (токенизация, стемминг), индексации и поиска. Ядро системы (индексация и поиск) реализовано на языке C++ с использованием собственных структур данных (вектор, хеш-таблица) в соответствии с требованиями курса.

\section{Лабораторная работа №1: Добыча корпуса документов}

\subsection{Цель работы}
Собрать и проанализировать корпус текстовых документов объемом не менее 30 000 статей для последующего использования в поисковой системе.

\subsection{Описание источников}
В качестве предметной области выбрана научная литература по биологии и медицине. Источниками данных послужили:
\begin{itemize}
    \item \textbf{PubMed Central (PMC)} — полнотекстовый архив биомедицинской литературы (формат XML/HTML). Собрано 28,999 статей.
    \item \textbf{Wikipedia} — энциклопедические статьи по генетике и биотехнологиям (CRISPR, редактирование генов). Собрано 3,810 статей.
\end{itemize}

Документы представляют собой научные статьи и энциклопедические тексты, содержащие заголовок, авторов (при наличии), аннотацию и основной текст. Язык документов — английский.

\subsection{Структура документа}
«Сырой» документ (HTML/XML) содержит:
\begin{itemize}
    \item Метаданные (DOI, журнал, дата публикации).
    \item Заголовок статьи (\texttt{<article-title>}, \texttt{<h1>}).
    \item Текст аннотации (\texttt{<abstract>}).
    \item Основной текст статьи, разбитый на секции (\texttt{<sec>}, \texttt{<p>}).
\end{itemize}

\subsection{Выделение текста}
Для выделения полезного контента использовался парсер, удаляющий служебные теги, скрипты, стили и навигационные элементы. В итоговый текстовый документ включаются:
\begin{enumerate}
    \item Заголовок.
    \item Аннотация.
    \item Основной текст статьи.
\end{enumerate}

\subsection{Статистика корпуса}
\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Параметр} & \textbf{Значение} \\
\hline
Количество документов (всего) & 32,809 \\
\hline
\quad из PubMed Central (PMC) & 28,999 \\
\hline
\quad из Wikipedia & 3,810 \\
\hline
Средний размер текста (на док.) & $\approx$ 12 КБ \\
\hline
Общий объем выделенного текста & $\approx$ 400 МБ \\
\hline
\end{tabular}
\caption{Статистика собранного корпуса}
\end{table}

\subsection{Проверка пригодности}
Поиск по источникам (PubMed, PMC) доступен через официальные веб-интерфейсы и API (E-utilities), что позволяет использовать данный корпус для сравнения результатов.

\section{Лабораторная работа №2: Поисковый робот}

\subsection{Цель работы}
Разработать поисковый робот (crawler) для автоматического сбора документов с поддержкой докачки, проверки свежести и соблюдения вежливости (задержки).

\subsection{Архитектура и стек технологий}
Робот реализован на языке \textbf{Python}.
Основные компоненты:
\begin{itemize}
    \item \textbf{YAML-конфигурация}: параметры запуска (seeds, timeout, user-agent) вынесены в отдельный файл.
    \item \textbf{MongoDB}: используется для хранения метаданных документов и очереди скачивания (frontier).
    \item \textbf{Requests}: библиотека для HTTP-запросов с поддержкой повторных попыток (retries).
\end{itemize}

\subsection{Алгоритм работы}
1. \textbf{Инициализация}: чтение конфига, подключение к БД.
2. \textbf{Очередь (Frontier)}: URL добавляются в очередь со статусом \texttt{pending}.
3. \textbf{Обход}:
    \begin{itemize}
        \item Взятие URL из очереди.
        \item Проверка \texttt{robots.txt} (опционально) и допустимых доменов.
        \item Загрузка \texttt{HEAD} или \texttt{GET} для проверки заголовков.
        \item Загрузка тела документа.
    \end{itemize}
4. \textbf{Обработка}:
    \begin{itemize}
        \item Вычисление хеша (SHA-256) контента.
        \item Проверка на изменения: если хеш совпадает с сохраненным, обновляется только поле \texttt{fetched\_at}.
        \item Парсинг: извлечение текста и ссылок.
        \item Сохранение: запись в MongoDB (\texttt{upsert}).
    \end{itemize}
5. \textbf{Докачка}: При перезапуске робот продолжает работу, выбирая из базы URL, которые еще не были посещены или требуют обновления.

\subsection{Результат}
Робот успешно собрал корпус документов. Реализована устойчивость к разрывам соединения и возможность остановки/продолжения работы без потери прогресса.

\section{Лабораторная работа №3: Токенизация}

\subsection{Цель работы}
Реализовать разбиение текста на токены (слова) для построения индекса.

\subsection{Правила токенизации}
В реализации на C++ использован следующий подход:
\begin{enumerate}
    \item \textbf{Разбиение}: текст сканируется посимвольно.
    \item \textbf{Алфавит}: токеном считается последовательность алфавитно-цифровых символов (\texttt{isalnum}).
    \item \textbf{Нормализация}: все символы приводятся к нижнему регистру (\texttt{tolower}).
    \item \textbf{Стемминг}: к каждому токену применяется алгоритм Портера для морфологической нормализации.
\end{enumerate}

\subsection{Достоинства метода}
\begin{itemize}
    \item \textbf{Простота}: минимальные требования к памяти и вычислениям.
    \item \textbf{Высокая скорость}: однопроходный алгоритм $O(n)$.
    \item \textbf{Универсальность}: работает с любым текстом на латинице.
    \item \textbf{Унификация словоформ}: стемминг объединяет различные формы слова.
\end{itemize}

\subsection{Недостатки метода}
\begin{itemize}
    \item \textbf{Потеря информации}: агрессивный стемминг может приводить к коллизиям (например, \texttt{using} $\to$ \texttt{us}, хотя \texttt{us} --- это местоимение).
    \item \textbf{Разбиение составных терминов}: дефисы и точки являются разделителями, поэтому \texttt{CRISPR-Cas9} становится двумя токенами \texttt{crispr} и \texttt{cas9}.
    \item \textbf{Числовые идентификаторы}: PMC-идентификаторы и DOI становятся отдельными токенами, засоряя словарь.
\end{itemize}

\subsection{Примеры неудачной токенизации}
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Исходный текст} & \textbf{Результат} & \textbf{Проблема} \\
\hline
CRISPR-Cas9 & crispr, cas9 & Разбито на два токена \\
\hline
using & us & Коллизия со словом «us» \\
\hline
10.1101/2023 & 10, 1101, 2023 & DOI разбит на числа \\
\hline
C. elegans & c, elegan & Потеря связи между родом и видом \\
\hline
\end{tabular}
\caption{Примеры неудачной токенизации}
\end{table}

\textbf{Возможные улучшения}:
\begin{itemize}
    \item Добавить словарь составных терминов (n-grams) для научной лексики.
    \item Использовать regex для распознавания DOI, идентификаторов.
    \item Применить лемматизацию вместо стемминга для снижения коллизий.
\end{itemize}

\subsection{Статистика токенизации}
\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Параметр} & \textbf{Значение} \\
\hline
Количество документов & 32,809 \\
\hline
Общее количество токенов & 385,103,087 \\
\hline
Уникальных токенов & 4,432,202 \\
\hline
Среднее токенов на документ & $\approx$ 11,738 \\
\hline
\end{tabular}
\caption{Статистика токенизации}
\end{table}

\subsection{Производительность}
\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Параметр} & \textbf{Значение} \\
\hline
Время токенизации (1000 файлов, 13.6 МБ) & 1.57 сек \\
\hline
Скорость токенизации & \textbf{8,897 КБ/сек} \\
\hline
\end{tabular}
\caption{Производительность токенизатора}
\end{table}

\textbf{Зависимость от объёма}: время линейно зависит от размера входных данных ($T = O(n)$), так как алгоритм однопроходный.

\textbf{Оптимальность}: скорость $\approx$ 9 МБ/сек является достаточной для данного объёма корпуса. Возможные способы ускорения:
\begin{itemize}
    \item Многопоточная обработка (параллельная токенизация файлов).
    \item SIMD-инструкции для обработки символов.
    \item Memory-mapped I/O для снижения накладных расходов на чтение файлов.
\end{itemize}

\section{Лабораторная работа №4: Стемминг}

\subsection{Цель работы}
Реализовать морфологическую нормализацию (стемминг) для улучшения полноты поиска.

\subsection{Метод решения}
Реализован \textbf{алгоритм Портера} (Porter Stemmer) для английского языка на C++. Алгоритм последовательно применяет ряд правил для отсечения суффиксов и окончаний.
Основные шаги алгоритма:
\begin{itemize}
    \item Step 1a: Обработка множественного числа (sses $\to$ ss, ies $\to$ i, s $\to$ \o).
    \item Step 1b: Обработка окончаний ed, ing.
    \item Step 1c: Замена y на i.
    \item Step 2-4: Замена длинных суффиксов (ational $\to$ ate, izer $\to$ ize и т.д.).
    \item Step 5: Удаление конечного e и двойных согласных (ll $\to$ l).
\end{itemize}

\subsection{Результаты}
Применение стемминга позволило сократить размер словаря примерно на 30-40\%, объединив различные словоформы (например, \textit{connect, connected, connecting, connection} $\to$ \textit{connect}) в один терм.

\subsection{Интеграция в поисковую систему}
Стемминг применяется в двух местах:
\begin{enumerate}
    \item \textbf{При индексации}: каждый токен документа проходит через Porter Stemmer перед добавлением в индекс.
    \item \textbf{При поиске}: запрос пользователя обрабатывается тем же стеммером (\texttt{TokenizerLib::tokenize()}).
\end{enumerate}

Это обеспечивает \textbf{поиск без учёта словоформ}: запрос «connected» найдёт документы, содержащие «connecting», «connection», «connects» и другие формы слова.

\textbf{Примеры унификации:}
\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Исходные слова} & \textbf{Результат стемминга} \\
\hline
connecting, connected, connection, connects & connect \\
\hline
running, ran, runs & run \\
\hline
studies, studying, studied & studi \\
\hline
generalization, generalize, general & gener \\
\hline
\end{tabular}
\caption{Примеры унификации словоформ}
\end{table}

\section{Лабораторная работа №5: Закон Ципфа}

\subsection{Цель работы}
Проверить выполнение закона Ципфа на собранном корпусе документов.

\subsection{Методика}
Для проверки закона был построен график распределения частот терминов в двойном логарифмическом масштабе (log-log plot):
\begin{itemize}
    \item По оси X: ранг слова (от самого частотного к редкому).
    \item По оси Y: частота встречаемости слова.
    \item Красная линия: теоретический закон Ципфа $f = \frac{C}{r}$, где $C$ --- частота самого частотного слова.
\end{itemize}

\subsection{Результаты}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{zipf_plot.png}
\caption{Распределение частот терминов (закон Ципфа)}
\end{figure}

\textbf{Топ-10 самых частотных слов:}
\begin{table}[H]
\centering
\begin{tabular}{|c|l|r|}
\hline
\textbf{Ранг} & \textbf{Терм} & \textbf{Частота} \\
\hline
1 & the & 12,121,168 \\
2 & and & 10,269,321 \\
3 & of & 9,951,984 \\
4 & in & 7,568,344 \\
5 & a & 5,350,599 \\
6 & to & 4,733,855 \\
7 & cell & 3,770,636 \\
8 & 10 & 3,715,038 \\
9 & for & 3,033,218 \\
10 & with & 2,903,543 \\
\hline
\end{tabular}
\caption{Топ-10 частотных терминов}
\end{table}

Согласно закону Ципфа: $f \approx \frac{C}{r^s}$, где $s \approx 1$.

Полученный график демонстрирует линейную зависимость с наклоном, близким к $-1$, что в целом подтверждает выполнение закона Ципфа для собранного корпуса научных текстов.

\subsection{Причины расхождений}
На графике наблюдаются характерные отклонения от идеального закона Ципфа:

\textbf{1. Область высоких рангов (частотные слова):}
\begin{itemize}
    \item Частота самых частотных слов (the, of, and) \textbf{ниже} предсказанной законом.
    \item Причина: эти слова --- стоп-слова, которые не несут специфичной информации и встречаются равномерно, а не концентрированно.
\end{itemize}

\textbf{2. Область низких рангов (редкие слова):}
\begin{itemize}
    \item Наблюдается \textbf{ступенчатость} --- много слов с частотой 1, 2, 3.
    \item Частота \textbf{выше} предсказанной (хвост толще).
    \item Причины:
    \begin{itemize}
        \item Научная терминология (специфические гены, белки).
        \item Опечатки и OCR-ошибки.
        \item Числовые идентификаторы (DOI, PMC-ID).
    \end{itemize}
\end{itemize}

\textbf{3. Средняя область:}
\begin{itemize}
    \item Хорошее соответствие закону Ципфа.
    \item Это типично для естественных текстов.
\end{itemize}

\subsection{Выводы}
Закон Ципфа выполняется для собранного корпуса с типичными отклонениями, характерными для научных текстов. Отклонения объясняются особенностями предметной области (биомедицина) и техническими артефактами (идентификаторы).

\section{Лабораторная работа №6: Булев индекс}

\subsection{Цель работы}
Построить обратный индекс для булева поиска, используя собственные реализации структур данных и бинарный формат хранения.

\subsection{Внутреннее представление документа}
После токенизации документ представляется как последовательность термов (стеммированных токенов). Каждый терм — строка в нижнем регистре. Для каждого терма хранится позиция в документе (порядковый номер токена).

\subsection{Реализация индексатора}
Индексатор написан на C++ без использования STL контейнеров (для хранения данных в памяти).
\subsubsection{Структуры данных}
\begin{itemize}
    \item \textbf{HashMap}: хеш-таблица с открытой адресацией для накопления словаря терминов и постинг-листов в памяти.
    \item \textbf{SimpleVector}: динамический массив для хранения списков doc\_id и позиций.
\end{itemize}

\subsubsection{Метод сортировки}
При записи индекса постинг-листы \textbf{не требуют явной сортировки}: документы обрабатываются последовательно (doc\_id = 0, 1, 2, ...), поэтому posting lists автоматически упорядочены по doc\_id.

\textbf{Достоинства:}
\begin{itemize}
    \item Нет накладных расходов на сортировку ($O(1)$ вместо $O(n \log n)$).
    \item Простота реализации.
\end{itemize}

\textbf{Недостатки:}
\begin{itemize}
    \item Требует последовательной обработки документов.
    \item При инкрементальном обновлении индекса потребуется merge-sort.
\end{itemize}

\subsubsection{Бинарный формат индекса (побайтовое описание)}
Индекс сохраняется в три файла:

\textbf{1. index.docs (Прямой индекс):}
\begin{verbatim}
[0-3]   Magic: "DOCS" (4 байта)
[4-5]   Version: uint16 (2 байта)
[6-9]   DocCount: uint32 (4 байта)
[10..]  OffsetTable: uint64[DocCount] (8 байт × DocCount)
[...]   Данные документов:
        - url_len: uint16 (2 байта)
        - url: char[url_len]
        - title_len: uint16 (2 байта)
        - title: char[title_len]
\end{verbatim}

\textbf{2. index.dict (Словарь термов):}
\begin{verbatim}
[0-3]   Magic: "DICT" (4 байта)
[4-5]   Version: uint16 (2 байта)
[6-9]   TermCount: uint32 (4 байта)
[...]   Записи термов:
        - term_len: uint8 (1 байт)
        - term: char[term_len]
        - post_offset: uint64 (8 байт) - смещение в index.postings
        - doc_freq: uint32 (4 байта) - количество документов
\end{verbatim}

\textbf{3. index.postings (Обратный индекс):}
\begin{verbatim}
[0-3]   Magic: "POST" (4 байта)
[4-5]   Version: uint16 (2 байта)
[...]   Постинг-листы (VarByte-сжатие):
        - doc_freq: VarByte
        Для каждого документа:
          - doc_id_delta: VarByte (разность с предыдущим)
          - term_freq: VarByte (частота в документе)
          - positions: VarByte[] (delta-кодированные позиции)
\end{verbatim}

\textbf{VarByte кодирование:} старший бит каждого байта — флаг продолжения (0 = последний байт, 1 = продолжение). Остальные 7 бит — данные.

\textbf{Расширяемость:} поле Version позволяет изменять формат в будущих версиях с сохранением обратной совместимости.

\subsection{Результаты}
\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Параметр} & \textbf{Значение} \\
\hline
Количество термов & 383,340 \\
\hline
Средняя длина терма & 7.93 символа \\
\hline
Средняя длина токена (до стемминга) & $\approx$ 5.5 символа \\
\hline
Размер index.dict & 7.52 МБ \\
\hline
Размер index.postings & 22.95 МБ \\
\hline
Общий размер индекса & 30.53 МБ \\
\hline
\end{tabular}
\caption{Статистика булева индекса}
\end{table}

\textbf{Сравнение длины терма и токена:} Средняя длина терма (7.93) \textbf{больше} средней длины токена ($\approx$5.5). Причина: короткие высокочастотные слова (a, the, is) имеют много вхождений, но в словаре каждый терм учитывается один раз. Также стемминг иногда увеличивает длину (iz $\to$ ize).

\subsection{Скорость индексации}
\begin{itemize}
    \item Общее время: $\approx$ 45 секунд (для 1000 документов).
    \item На документ: $\approx$ 45 мс.
    \item На килобайт текста: $\approx$ 3 мс.
\end{itemize}

\subsection{Анализ оптимальности}
\textbf{Текущие ограничения:}
\begin{itemize}
    \item Индекс строится в памяти — ограничен объёмом RAM.
    \item Однопоточная обработка.
\end{itemize}

\textbf{Возможные оптимизации:}
\begin{itemize}
    \item Многопоточная токенизация (параллельная обработка документов).
    \item Внешняя сортировка (external merge sort) для больших корпусов.
    \item Memory-mapped I/O для больших файлов.
\end{itemize}

\textbf{Масштабируемость:}
\begin{itemize}
    \item \textbf{10× данных}: справится, потребуется $\approx$ 300 МБ RAM.
    \item \textbf{100× данных}: потребуется внешняя сортировка или распределённая индексация.
    \item \textbf{1000× данных}: необходима распределённая система (MapReduce, Spark).
\end{itemize}

\section{Лабораторная работа №7: Булев поиск}

\subsection{Цель работы}
Реализовать выполнение булевых запросов (AND, OR, NOT) по построенному индексу с поддержкой веб-интерфейса и командной строки.

\subsection{Синтаксис запросов}
Парсер поддерживает следующий синтаксис:
\begin{itemize}
    \item \textbf{Пробел или \&\&} — логическое И (пересечение).
    \item \textbf{||} — логическое ИЛИ (объединение).
    \item \textbf{!} — логическое НЕ (исключение).
    \item \textbf{Скобки} — группировка операций.
\end{itemize}

Парсер реализован методом \textbf{рекурсивного спуска} и устойчив к переменному числу пробелов.

\subsection{Алгоритм поиска}
Поисковый движок (C++) работает следующим образом:
\begin{enumerate}
    \item \textbf{Загрузка}: Словарь загружается в HashMap, постинг-листы — в память.
    \item \textbf{Токенизация запроса}: Выделение термов и операторов. Термы проходят стемминг.
    \item \textbf{Парсинг}: Построение дерева выражения (рекурсивный спуск).
    \item \textbf{Выполнение}:
    \begin{itemize}
        \item \textbf{AND}: Пересечение сортированных списков (два указателя, $O(N+M)$).
        \item \textbf{OR}: Слияние списков ($O(N+M)$).
        \item \textbf{NOT}: Разность с множеством всех документов.
    \end{itemize}
\end{enumerate}

\subsection{Примеры запросов}
\begin{table}[H]
\centering
\begin{tabular}{|l|c|l|}
\hline
\textbf{Запрос} & \textbf{Результат} & \textbf{Описание} \\
\hline
\texttt{cell} & 971 док. & Простой поиск \\
\hline
\texttt{cell \&\& protein} & 898 док. & Пересечение (AND) \\
\hline
\texttt{cell || virus} & 974 док. & Объединение (OR) \\
\hline
\texttt{!cell} & 29 док. & Исключение (NOT) \\
\hline
\texttt{(gene || protein) \&\& cell} & 949 док. & Сложный запрос со скобками \\
\hline
\end{tabular}
\caption{Примеры булевых запросов}
\end{table}

\subsection{Веб-интерфейс}
Реализован веб-сервис на Flask:
\begin{itemize}
    \item \textbf{Главная страница} (\texttt{/}): Форма ввода поискового запроса.
    \item \textbf{Страница результатов} (\texttt{/search?q=...}): Форма поиска, количество результатов, время выполнения, список из 50 результатов (заголовок + ссылка).
\end{itemize}

\subsection{CLI-утилита}
Реализована утилита командной строки \texttt{bin/search <index\_dir>}, которая:
\begin{itemize}
    \item Загружает индекс из указанной директории.
    \item Читает запросы из stdin (по одному на строку).
    \item Выводит результаты в stdout.
\end{itemize}

\subsection{Производительность}
\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Метрика} & \textbf{Значение} \\
\hline
Загрузка индекса & $\approx$ 50 мс \\
\hline
Простой запрос (1 терм) & $<$ 1 мс \\
\hline
Сложный запрос (AND/OR/NOT) & $\approx$ 2-5 мс \\
\hline
5 запросов подряд (без загрузки) & $\approx$ 40 мс \\
\hline
\end{tabular}
\caption{Производительность поисковой системы}
\end{table}

\textbf{Примеры долгих запросов:}
\begin{itemize}
    \item Запросы с NOT на частотных термах (\texttt{!the}) — требуют обхода всех документов.
    \item Множественные OR (\texttt{a || b || c || ...}) — большие объединения.
\end{itemize}

\subsection{Тестирование корректности}
Корректность проверялась следующими методами:
\begin{enumerate}
    \item \textbf{Инвариант NOT}: $|A| + |!A| = N$ (общее число документов).
    \item \textbf{Инвариант AND}: $|A \cap B| \leq \min(|A|, |B|)$.
    \item \textbf{Инвариант OR}: $|A \cup B| \geq \max(|A|, |B|)$.
    \item \textbf{Сравнение с grep}: Выборочная проверка результатов поиска путём поиска терма в исходных текстах.
\end{enumerate}

\section{Выводы}
В ходе лабораторных работ была создана поисковая система полного цикла. Реализация ядра на C++ без использования тяжелых библиотек (STL) и применение эффективных методов сжатия (VarByte) позволили достичь высокой производительности и компактности индекса. Система успешно справляется с индексацией и поиском по корпусу из \textbf{32,809 документов} (PMC + Wikipedia), содержащему \textbf{385 миллионов токенов}.

\end{document}
